{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules \n",
    "import sys\n",
    "from os import getcwd\n",
    "from os.path import dirname\n",
    "path = dirname(dirname(getcwd()))\n",
    "sys.path.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MintPy.interpret_toolkit import InterpretToolkit\n",
    "from MintPy.utils import combine_top_features\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/deep/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/anaconda3/envs/deep/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.tree.tree module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.tree. Anything that cannot be imported from sklearn.tree is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define target feature\n",
    "TARGET_COLUMN = 'cat_rt'\n",
    "\n",
    "# Load the model objects. In this case, we are using 3 \n",
    "# popular models availabe in scikit-learn \n",
    "model_fname = ['RandomForest.pkl', 'LogisticRegression.pkl', 'GradientBoostingClassifier.pkl']\n",
    "model_objs = [load(fname) for fname in model_fname]\n",
    "\n",
    "# Load the training dataset \n",
    "data  = pd.read_csv('example_data.csv')\n",
    "targets = data[TARGET_COLUMN].values\n",
    "\n",
    "# only want to use these columns below\n",
    "cols_to_use = ['dllwave_flux', 'dwpt2m', 'fric_vel', 'gflux', 'high_cloud',\n",
    "            'lat_hf', 'low_cloud', 'mid_cloud', 'sat_irbt', 'sens_hf',\n",
    "            'sfcT_hrs_ab_frez', 'sfcT_hrs_bl_frez', 'sfc_rough', 'sfc_temp',\n",
    "            'swave_flux','temp2m', 'tmp2m_hrs_ab_frez', 'tmp2m_hrs_bl_frez',\n",
    "            'tot_cloud', 'uplwav_flux','vbd_flux', 'vdd_flux','wind10m',\n",
    "            'date_marker', 'urban','rural','d_ground','d_rad_d','d_rad_u',\n",
    "            'hrrr_dT']\n",
    "\n",
    "units = ['W m$^{-2}$', '$^\\circ$C', 'm s$^{-1}$', 'W m$^{-2}$', '%', 'W m$^{-2}$', '%', '%', \n",
    "         '$^\\circ$C', 'W m$^{-2}$', 'hrs', 'hrs', 'unitless','$^\\circ$C', 'W m$^{-2}$', '$^\\circ$C', \n",
    "         'hrs', 'hrs', '%', 'W m$^{-2}$', 'W m$^{-2}$', 'W m$^{-2}$', 'm s$^{-1}$', 'days', 'unitless', \n",
    "         'unitless', 'W m$^{-2}$', 'W m$^{-2}$', 'W m$^{-2}$', '$^\\circ$C']\n",
    "\n",
    "pretty_names = [r'$\\lambda_{\\downarrow}$', '$T_{d}$', '$V_{fric}$', 'Gflux', '$Cloud_{high}$',\n",
    " '$Lat_{F}$', '$Cloud_{low}$', '$Cloud_{mid}$', 'IRBT', '$Sens_{F}$',\n",
    " 'Hours $T_{sfc}$ $> $0', 'Hours $T_{sfc}$ $<= $0', 'SfcRough', '$T_{sfc}$',\n",
    " '$I_{S}$', '$T_{2m}$', 'Hours $T_{2m}$ $> $0', 'Hours $T_{2m}$ $<= $0',\n",
    " '$Cloud_{Tot}$', r'$\\lambda_{\\uparrow}$', 'VBD', 'VDD', '10m wind',\n",
    " 'Date marker', 'Urban', 'Rural', 'Diff1', 'Diff2', 'Diff3',\n",
    " '$T_{sfc}$ - $T_{2m}$']\n",
    "\n",
    "feature_units = {c : u for c,u in zip(cols_to_use, units)}\n",
    "readable_feature_names = {c : u for c,u in zip(cols_to_use, pretty_names)}\n",
    "\n",
    "# get predictor subset of dataframe (only the predictors used in training the model)\n",
    "examples = data[cols_to_use]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing InterpretToolkit\n",
    "\n",
    "To initialize `InterpretToolkit`, requires a model object (e.g., a trained sci-kit learn model object) or a list of model objects and examples and targets to evalute the model(s) on. `examples` and `targets` can be `pandas.DataFrame` or `numpy.array`, but if you are using arrays, then you must provide the feature names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "myInterpreter = InterpretToolkit(model=model_objs, \n",
    "                             examples=examples, \n",
    "                             targets=targets,\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation Importance\n",
    "\n",
    "A first step in model interpretability is determining feature ranking. A popular model-agnostic method for determining predictor ranking is the permutation importance method. The permutation importance calculations in MintPy are performed by a stripped-down version of PermutationImportance (see https://permutationimportance.readthedocs.io/en/latest/ for additional details).  \n",
    " \n",
    "Additional options for permutation importance include: \n",
    "* Performing bootstraping for confidence interval on predictor ranking (`nbootstrap`)\n",
    "* Using multiple processors for parallelization (`njobs`)\n",
    "* Subsample examples for speedier results (`subsample`)\n",
    "\n",
    "We currently have 3 built-in error metrics for evaluating feature importance\n",
    "* Area under the Curve (`'auc'`)\n",
    "* Area under the Performance Diagram (`'aupdc'`)\n",
    "* Brier Skill Score (`'bss'`)\n",
    "\n",
    "In this example, we want the top 5 predictors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing RandomForestClassifier...\n",
      "Using 3 of processors to compute importance...\n",
      "Starting on the important variable 1 out of 5...\n",
      "Starting on the important variable 2 out of 5...\n",
      "Starting on the important variable 3 out of 5...\n",
      "Starting on the important variable 4 out of 5...\n",
      "Starting on the important variable 5 out of 5...\n",
      "Processing LogisticRegression...\n",
      "Using 3 of processors to compute importance...\n",
      "Starting on the important variable 1 out of 5...\n",
      "Starting on the important variable 2 out of 5...\n",
      "Starting on the important variable 3 out of 5...\n",
      "Starting on the important variable 4 out of 5...\n",
      "Starting on the important variable 5 out of 5...\n",
      "Processing GradientBoostingClassifier...\n",
      "Using 3 of processors to compute importance...\n",
      "Starting on the important variable 1 out of 5...\n",
      "Starting on the important variable 2 out of 5...\n",
      "Starting on the important variable 3 out of 5...\n",
      "Starting on the important variable 4 out of 5...\n",
      "Starting on the important variable 5 out of 5...\n"
     ]
    }
   ],
   "source": [
    "results = myInterpreter.permutation_importance(\n",
    "                                               n_vars=5, \n",
    "                                               evaluation_fn='auc', \n",
    "                                               nbootstrap=10, \n",
    "                                               subsample = 1.0,\n",
    "                                               njobs=3\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'RandomForestClassifier': ['dllwave_flux', 'dwpt2m', 'sfc_temp', 'temp2m', 'sfcT_hrs_ab_frez'], 'LogisticRegression': ['tmp2m_hrs_ab_frez', 'swave_flux', 'sfcT_hrs_ab_frez', 'sfcT_hrs_bl_frez', 'dwpt2m'], 'GradientBoostingClassifier': ['sfc_temp', 'dwpt2m', 'temp2m', 'high_cloud', 'tmp2m_hrs_bl_frez']}\n"
     ]
    }
   ],
   "source": [
    "# There is a built-in method to return a list of the top features for each model\n",
    "important_vars = myInterpreter.get_important_vars(results, multipass=True)\n",
    "print(important_vars)\n",
    "\n",
    "# Since permutation importance can be a time-consuming for multiple predictors over a large dataset,\n",
    "# MintPy also have a built-in function for saving the results \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# And a function for reloading the file and appropriately setting it as a class attribute \n",
    "# for the plotting function. \n",
    "\n",
    "important_vars = combine_top_features(important_vars, nvars=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = myInterpreter.plot_importance(multipass=True, \n",
    "                              metric = \"Training AUC\",\n",
    "                              num_vars_to_plot=5)\n",
    "\n",
    "# Saving the figures\n",
    "myInterpreter.save_figure(fig=fig, \n",
    "                          fname='multipass_perm_importance.png', \n",
    "                          bbox_inches=\"tight\", \n",
    "                          dpi=300, aformat=\"png\")\n",
    "\n",
    "# Fix the axis labelling for cases with only one panel!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
